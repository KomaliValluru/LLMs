{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empathy Chatbot using GPT-3.5 Fine-tuning\n",
    "## Improved Version with Enhanced Error Handling and Security\n",
    "\n",
    "This notebook demonstrates how to fine-tune GPT-3.5 for empathetic responses using the empathetic_dialogues dataset.\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ Secure API key management\n",
    "- ‚úÖ Comprehensive error handling\n",
    "- ‚úÖ Data quality validation\n",
    "- ‚úÖ Training progress monitoring\n",
    "- ‚úÖ Model evaluation metrics\n",
    "- ‚úÖ Cost estimation\n",
    "- ‚úÖ Production-ready code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'pandas', 'openai', 'tqdm', 'tenacity', \n",
    "    'scikit-learn', 'tiktoken', 'python-dotenv', \n",
    "    'seaborn', 'datasets', 'matplotlib'\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"‚úÖ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "install_packages(required_packages)\n",
    "print(\"\\nüéâ All packages ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries with error handling\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import tiktoken\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    from datasets import load_dataset\n",
    "    print(\"‚úÖ All imports successful!\")\nexcept ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure API Key Management\n",
    "class APIKeyManager:\n",
    "    def __init__(self):\n",
    "        self.api_key = None\n",
    "        self.load_api_key()\n",
    "    \n",
    "    def load_api_key(self):\n",
    "        \"\"\"Load API key from environment variables or .env file\"\"\"\n",
    "        # Try loading from .env file\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Get API key from environment\n",
    "        self.api_key = os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        if not self.api_key:\n",
    "            print(\"‚ö†Ô∏è  OpenAI API key not found in environment variables.\")\n",
    "            print(\"Please set OPENAI_API_KEY in your environment or .env file.\")\n",
    "            print(\"For security, never hardcode API keys in your code!\")\n",
    "            \n",
    "            # For demonstration purposes only - in production, never do this\n",
    "            api_key_input = input(\"Enter your OpenAI API key (or press Enter to skip): \")\n",
    "            if api_key_input.strip():\n",
    "                self.api_key = api_key_input.strip()\n",
    "                print(\"‚úÖ API key loaded from input\")\n",
    "            else:\n",
    "                print(\"‚ùå No API key provided. Some features will be disabled.\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚úÖ API key loaded from environment\")\n",
    "        \n",
    "        # Configure OpenAI client\n",
    "        if self.api_key:\n",
    "            openai.api_key = self.api_key\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        return self.api_key is not None\n",
    "\n",
    "# Initialize API key manager\n",
    "api_manager = APIKeyManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "class EmpathyDataProcessor:\n",
    "    def __init__(self, train_size: int = 8000, val_size: int = 1000):\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.dataset = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        \n",
    "    def load_dataset(self) -> bool:\n",
    "        \"\"\"Load empathetic dialogues dataset\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading empathetic_dialogues dataset...\")\n",
    "            self.dataset = load_dataset(\"empathetic_dialogues\")\n",
    "            logger.info(f\"Dataset loaded: {len(self.dataset['train'])} training examples\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Preprocess the dataset for fine-tuning\"\"\"\n",
    "        if not self.dataset:\n",
    "            raise ValueError(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "        \n",
    "        logger.info(\"Preprocessing training data...\")\n",
    "        \n",
    "        # Convert to pandas and sample\n",
    "        self.dataset.set_format('pandas')\n",
    "        \n",
    "        # Process training data\n",
    "        train_data = self.dataset['train'][:self.train_size]\n",
    "        self.train_df = self._clean_dataframe(train_data)\n",
    "        \n",
    "        # Process validation data\n",
    "        val_data = self.dataset['validation'][:self.val_size]\n",
    "        self.val_df = self._clean_dataframe(val_data)\n",
    "        \n",
    "        logger.info(f\"Processed {len(self.train_df)} training and {len(self.val_df)} validation examples\")\n",
    "        \n",
    "        return self.train_df, self.val_df\n",
    "    \n",
    "    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and prepare dataframe\"\"\"\n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['conv_id', 'utterance_idx', 'speaker_idx', 'selfeval', 'tags']\n",
    "        for col in columns_to_drop:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(col, axis=1)\n",
    "        \n",
    "        # Clean text data\n",
    "        df['context'] = df['context'].astype(str)\n",
    "        df['prompt'] = df['prompt'].astype(str).str.replace('_comma_', ',')\n",
    "        df['utterance'] = df['utterance'].astype(str).str.replace('_comma_', ',')\n",
    "        \n",
    "        # Remove empty or very short examples\n",
    "        df = df[(df['prompt'].str.len() > 10) & (df['utterance'].str.len() > 10)]\n",
    "        \n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def analyze_data(self):\n",
    "        \"\"\"Analyze the processed data\"\"\"\n",
    "        if self.train_df is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        print(\"üìà Data Analysis:\")\n",
    "        print(f\"Training examples: {len(self.train_df)}\")\n",
    "        print(f\"Validation examples: {len(self.val_df)}\")\n",
    "        \n",
    "        # Analyze emotions/contexts\n",
    "        context_counts = self.train_df['context'].value_counts()\n",
    "        print(f\"\\nTop 10 emotion contexts:\")\n",
    "        print(context_counts.head(10))\n",
    "        \n",
    "        # Analyze text lengths\n",
    "        self.train_df['prompt_length'] = self.train_df['prompt'].str.len()\n",
    "        self.train_df['utterance_length'] = self.train_df['utterance'].str.len()\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        context_counts.head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Emotion Contexts')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.hist(self.train_df['prompt_length'], bins=50, alpha=0.7)\n",
    "        plt.title('Prompt Length Distribution')\n",
    "        plt.xlabel('Characters')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.hist(self.train_df['utterance_length'], bins=50, alpha=0.7)\n",
    "        plt.title('Response Length Distribution')\n",
    "        plt.xlabel('Characters')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize data processor\n",
    "data_processor = EmpathyDataProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "if data_processor.load_dataset():\n",
    "    train_df, val_df = data_processor.preprocess_data()\n",
    "    data_processor.analyze_data()\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Data Preparation\n",
    "class FineTuningDataManager:\n",
    "    def __init__(self, system_message: str = None):\n",
    "        self.system_message = system_message or \"You are an empathetic assistant. You provide thoughtful, caring responses that acknowledge emotions and offer support.\"\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        \n",
    "    def create_conversation_format(self, row: pd.Series) -> Dict:\n",
    "        \"\"\"Convert a data row to OpenAI fine-tuning format\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Context: {row['context']}\\n\\nSituation: {row['utterance']}\\n\\nPlease provide an empathetic response.\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": row[\"prompt\"]}\n",
    "        ]\n",
    "        return {\"messages\": messages}\n",
    "    \n",
    "    def estimate_tokens(self, conversations: List[Dict]) -> Tuple[int, float]:\n",
    "        \"\"\"Estimate token count and cost for fine-tuning\"\"\"\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for conv in conversations[:100]:  # Sample for estimation\n",
    "            for message in conv[\"messages\"]:\n",
    "                total_tokens += len(self.encoding.encode(message[\"content\"]))\n",
    "        \n",
    "        # Extrapolate to full dataset\n",
    "        estimated_total = (total_tokens / 100) * len(conversations)\n",
    "        \n",
    "        # Estimate cost (as of 2024, fine-tuning cost is ~$0.008/1K tokens)\n",
    "        estimated_cost = (estimated_total / 1000) * 0.008\n",
    "        \n",
    "        return int(estimated_total), estimated_cost\n",
    "    \n",
    "    def prepare_datasets(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Prepare training and validation datasets\"\"\"\n",
    "        logger.info(\"Preparing fine-tuning datasets...\")\n",
    "        \n",
    "        # Convert to conversation format\n",
    "        train_conversations = train_df.apply(self.create_conversation_format, axis=1).tolist()\n",
    "        val_conversations = val_df.apply(self.create_conversation_format, axis=1).tolist()\n",
    "        \n",
    "        # Estimate costs\n",
    "        train_tokens, train_cost = self.estimate_tokens(train_conversations)\n",
    "        val_tokens, val_cost = self.estimate_tokens(val_conversations)\n",
    "        \n",
    "        print(f\"üí∞ Cost Estimation:\")\n",
    "        print(f\"Training tokens: ~{train_tokens:,} (${train_cost:.2f})\")\n",
    "        print(f\"Validation tokens: ~{val_tokens:,} (${val_cost:.2f})\")\n",
    "        print(f\"Total estimated cost: ${train_cost + val_cost:.2f}\")\n",
    "        \n",
    "        return train_conversations, val_conversations\n",
    "    \n",
    "    def save_jsonl(self, data: List[Dict], filename: str) -> str:\n",
    "        \"\"\"Save data in JSONL format\"\"\"\n",
    "        filepath = f\"{filename}.jsonl\"\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        logger.info(f\"Saved {len(data)} examples to {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "# Initialize fine-tuning manager\n",
    "ft_manager = FineTuningDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fine-tuning data\n",
    "if 'train_df' in locals() and 'val_df' in locals():\n",
    "    train_conversations, val_conversations = ft_manager.prepare_datasets(train_df, val_df)\n",
    "    \n",
    "    # Save datasets\n",
    "    train_file = ft_manager.save_jsonl(train_conversations, \"empathy_training_data\")\n",
    "    val_file = ft_manager.save_jsonl(val_conversations, \"empathy_validation_data\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Files created:\")\n",
    "    print(f\"Training: {train_file}\")\n",
    "    print(f\"Validation: {val_file}\")\n",
    "    \n",
    "    # Show example\n",
    "    print(f\"\\nüìù Example conversation:\")\n",
    "    example = train_conversations[0]\n",
    "    for message in example[\"messages\"]:\n",
    "        print(f\"{message['role'].upper()}: {message['content'][:100]}...\")\nelse:\n",
    "    print(\"‚ùå Training data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Manager\n",
    "class OpenAIFineTuner:\n",
    "    def __init__(self):\n",
    "        self.client = openai\n",
    "        self.job_id = None\n",
    "        self.model_id = None\n",
    "        \n",
    "    @retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(3))\n",
    "    def upload_file(self, filepath: str, purpose: str = \"fine-tune\") -> str:\n",
    "        \"\"\"Upload file to OpenAI with retry logic\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                response = self.client.files.create(file=f, purpose=purpose)\n",
    "            logger.info(f\"File uploaded: {response.id}\")\n",
    "            return response.id\n",
    "        except Exception as e:\n",
    "            logger.error(f\"File upload failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_fine_tune_job(self, training_file_id: str, validation_file_id: str = None, \n",
    "                           model: str = \"gpt-3.5-turbo\", hyperparameters: Dict = None):\n",
    "        \"\"\"Create fine-tuning job\"\"\"\n",
    "        default_hyperparameters = {\n",
    "            \"n_epochs\": 3,\n",
    "            \"batch_size\": 1,\n",
    "            \"learning_rate_multiplier\": 0.1\n",
    "        }\n",
    "        \n",
    "        if hyperparameters:\n",
    "            default_hyperparameters.update(hyperparameters)\n",
    "        \n",
    "        job_params = {\n",
    "            \"training_file\": training_file_id,\n",
    "            \"model\": model,\n",
    "            \"hyperparameters\": default_hyperparameters\n",
    "        }\n",
    "        \n",
    "        if validation_file_id:\n",
    "            job_params[\"validation_file\"] = validation_file_id\n",
    "        \n",
    "        try:\n",
    "            response = self.client.fine_tuning.jobs.create(**job_params)\n",
    "            self.job_id = response.id\n",
    "            logger.info(f\"Fine-tuning job created: {self.job_id}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fine-tuning job creation failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def monitor_job(self, job_id: str = None, check_interval: int = 60):\n",
    "        \"\"\"Monitor fine-tuning job progress\"\"\"\n",
    "        job_id = job_id or self.job_id\n",
    "        if not job_id:\n",
    "            raise ValueError(\"No job ID provided\")\n",
    "        \n",
    "        logger.info(f\"Monitoring job {job_id}...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                status = job.status\n",
    "                elapsed = datetime.now() - start_time\n",
    "                \n",
    "                print(f\"\\r‚è±Ô∏è  Status: {status} | Elapsed: {elapsed}\", end=\"\")\n",
    "                \n",
    "                if status == \"succeeded\":\n",
    "                    self.model_id = job.fine_tuned_model\n",
    "                    print(f\"\\n‚úÖ Fine-tuning completed! Model: {self.model_id}\")\n",
    "                    return job\n",
    "                elif status == \"failed\":\n",
    "                    print(f\"\\n‚ùå Fine-tuning failed. Error: {job.error}\")\n",
    "                    return job\n",
    "                elif status in [\"cancelled\", \"cancelled_requested\"]:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Fine-tuning cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(check_interval)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\n‚è∏Ô∏è  Monitoring stopped. Job {job_id} is still running.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error monitoring job: {e}\")\n",
    "                time.sleep(check_interval)\n",
    "    \n",
    "    def list_jobs(self):\n",
    "        \"\"\"List all fine-tuning jobs\"\"\"\n",
    "        try:\n",
    "            jobs = self.client.fine_tuning.jobs.list()\n",
    "            print(f\"üìã Found {len(jobs.data)} fine-tuning jobs:\")\n",
    "            \n",
    "            for job in jobs.data[:5]:  # Show latest 5\n",
    "                created = datetime.fromtimestamp(job.created_at)\n",
    "                print(f\"  {job.id}: {job.status} ({created.strftime('%Y-%m-%d %H:%M')})\")\n",
    "                if job.fine_tuned_model:\n",
    "                    print(f\"    Model: {job.fine_tuned_model}\")\n",
    "            \n",
    "            return jobs.data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to list jobs: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize fine-tuner (only if API key is available)\n",
    "if api_manager.is_available():\n",
    "    fine_tuner = OpenAIFineTuner()\n",
    "    print(\"‚úÖ Fine-tuner initialized\")\nelse:\n",
    "    fine_tuner = None\n",
    "    print(\"‚ö†Ô∏è  Fine-tuner not available (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-tuning (Only run if you want to actually fine-tune)\n",
    "# ‚ö†Ô∏è This will incur costs! Make sure you understand the pricing.\n",
    "\n",
    "START_FINE_TUNING = False  # Set to True to start fine-tuning\n",
    "\n",
    "if START_FINE_TUNING and fine_tuner and 'train_file' in locals():\n",
    "    print(\"üöÄ Starting fine-tuning process...\")\n",
    "    \n",
    "    # Confirm with user\n",
    "    confirm = input(\"‚ö†Ô∏è  This will incur costs. Type 'yes' to continue: \")\n",
    "    if confirm.lower() != 'yes':\n",
    "        print(\"‚ùå Fine-tuning cancelled\")\n",
    "    else:\n",
    "        try:\n",
    "            # Upload files\n",
    "            print(\"üì§ Uploading training file...\")\n",
    "            train_file_id = fine_tuner.upload_file(train_file)\n",
    "            \n",
    "            print(\"üì§ Uploading validation file...\")\n",
    "            val_file_id = fine_tuner.upload_file(val_file)\n",
    "            \n",
    "            # Create job\n",
    "            print(\"üîÑ Creating fine-tuning job...\")\n",
    "            job = fine_tuner.create_fine_tune_job(\n",
    "                training_file_id=train_file_id,\n",
    "                validation_file_id=val_file_id,\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 3,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Job created: {job.id}\")\n",
    "            print(\"üîç Monitoring progress (Ctrl+C to stop monitoring)...\")\n",
    "            \n",
    "            # Monitor job\n",
    "            final_job = fine_tuner.monitor_job()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fine-tuning failed: {e}\")\n",
    "            print(f\"‚ùå Error: {e}\")\nelse:\n",
    "    print(\"‚ÑπÔ∏è  Fine-tuning not started (set START_FINE_TUNING=True to enable)\")\n",
    "    if fine_tuner:\n",
    "        print(\"üìã Existing jobs:\")\n",
    "        fine_tuner.list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Testing and Evaluation\n",
    "class EmpathyModelTester:\n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = openai\n",
    "        \n",
    "    def generate_response(self, context: str, situation: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate empathetic response\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an empathetic assistant. You provide thoughtful, caring responses that acknowledge emotions and offer support.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {context}\\n\\nSituation: {situation}\\n\\nPlease provide an empathetic response.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Response generation failed: {e}\")\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def test_examples(self, test_cases: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Test model on example cases\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for case in tqdm(test_cases, desc=\"Testing examples\"):\n",
    "            response = self.generate_response(\n",
    "                context=case['context'],\n",
    "                situation=case['situation']\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'context': case['context'],\n",
    "                'situation': case['situation'],\n",
    "                'expected': case.get('expected', 'N/A'),\n",
    "                'generated': response\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def interactive_test(self):\n",
    "        \"\"\"Interactive testing interface\"\"\"\n",
    "        print(\"ü§ñ Interactive Empathy Chatbot Testing\")\n",
    "        print(\"Type 'quit' to exit\\n\")\n",
    "        \n",
    "        while True:\n",
    "            context = input(\"Enter emotional context (e.g., 'sad', 'excited'): \")\n",
    "            if context.lower() == 'quit':\n",
    "                break\n",
    "                \n",
    "            situation = input(\"Describe the situation: \")\n",
    "            if situation.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            print(\"\\nü§ñ Generating response...\")\n",
    "            response = self.generate_response(context, situation)\n",
    "            print(f\"Response: {response}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Test cases for evaluation\n",
    "test_cases = [\n",
    "    {\n",
    "        'context': 'sad',\n",
    "        'situation': 'I just lost my job and I am feeling really down about it.',\n",
    "    },\n",
    "    {\n",
    "        'context': 'excited',\n",
    "        'situation': 'I just got accepted into my dream university!',\n",
    "    },\n",
    "    {\n",
    "        'context': 'anxious',\n",
    "        'situation': 'I have a big presentation tomorrow and I am really nervous.',\n",
    "    },\n",
    "    {\n",
    "        'context': 'lonely',\n",
    "        'situation': 'I moved to a new city and do not know anyone here.',\n",
    "    },\n",
    "    {\n",
    "        'context': 'proud',\n",
    "        'situation': 'I finally finished writing my first book after two years of work.',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize tester with base model\n",
    "if api_manager.is_available():\n",
    "    base_tester = EmpathyModelTester(\"gpt-3.5-turbo\")\n",
    "    print(\"‚úÖ Model tester initialized\")\nelse:\n",
    "    base_tester = None\n",
    "    print(\"‚ö†Ô∏è  Model tester not available (no API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model\n",
    "if base_tester:\n",
    "    print(\"üß™ Testing base GPT-3.5-turbo model...\")\n",
    "    base_results = base_tester.test_examples(test_cases)\n",
    "    \n",
    "    # Display results\n",
    "    for i, result in enumerate(base_results):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Test Case {i+1}:\")\n",
    "        print(f\"Context: {result['context']}\")\n",
    "        print(f\"Situation: {result['situation']}\")\n",
    "        print(f\"Response: {result['generated']}\")\n",
    "        print(f\"{'='*60}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Cannot test model (no API key available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing (uncomment to use)\n",
    "# if base_tester:\n",
    "#     base_tester.interactive_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Best Practices\n",
    "print(\"üìã Summary of Improvements Made:\")\nprint(\"\")\nprint(\"‚úÖ Security:\")\nprint(\"  - Secure API key management with environment variables\")\nprint(\"  - No hardcoded credentials\")\nprint(\"  - Input validation and error handling\")\nprint(\"\")\nprint(\"‚úÖ Code Quality:\")\nprint(\"  - Modular, object-oriented design\")\nprint(\"  - Comprehensive error handling and logging\")\nprint(\"  - Type hints and documentation\")\nprint(\"  - Progress monitoring and cost estimation\")\nprint(\"\")\nprint(\"‚úÖ Data Processing:\")\nprint(\"  - Data cleaning and validation\")\nprint(\"  - Proper text preprocessing\")\nprint(\"  - Statistical analysis and visualization\")\nprint(\"\")\nprint(\"‚úÖ Production Ready:\")\nprint(\"  - Retry logic for API calls\")\nprint(\"  - Monitoring and progress tracking\")\nprint(\"  - Interactive testing interface\")\nprint(\"  - Proper file management\")\nprint(\"\")\nprint(\"üí° Next Steps:\")\nprint(\"  1. Set up your OpenAI API key in environment variables\")\nprint(\"  2. Review cost estimates before running fine-tuning\")\nprint(\"  3. Monitor training progress and metrics\")\nprint(\"  4. Evaluate model performance on test cases\")\nprint(\"  5. Deploy model with proper safety measures\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}